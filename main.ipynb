{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import time\n",
    "import pandas as pd\n",
    "import math\n",
    "from utils.data_utils import *\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import model libs\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Conv1D, Dropout, Flatten, MaxPooling1D, BatchNormalization, LSTM\n",
    "# from keras import optimizers\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "\n",
    "\n",
    "fs = 125\n",
    "minBPM = 40\n",
    "maxBPM = 240\n",
    "window_length = 8 * fs\n",
    "window_shift = 2 * fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 1000)\n",
      "(148,)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve dataset files\n",
    "data_dir = \"datasets/troika/training_data\"\n",
    "data_fls, ref_fls = LoadTroikaDataset(data_dir)\n",
    "errs, confs = [], []\n",
    "\n",
    "data_fl = data_fls[0]\n",
    "ref_fl = ref_fls[0]\n",
    "\n",
    "# load data using LoadTroikaDataFile\n",
    "ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "\n",
    "# bandpass filter the signals\n",
    "ppg = bandpass_filter(ppg, fs)\n",
    "accx = bandpass_filter(accx, fs)\n",
    "accy = bandpass_filter(accy, fs)\n",
    "accz = bandpass_filter(accz, fs)\n",
    "\n",
    "# Consider only magnitude of acceleration\n",
    "acc = calculate_magnitude(accx, accy, accz)\n",
    "\n",
    "# Standardization\n",
    "ppg = (ppg- np.mean(ppg))/np.std(ppg)\n",
    "acc = (acc- np.mean(acc))/np.std(acc)\n",
    "\n",
    "# loading the reference file\n",
    "ground_truth = sp.io.loadmat(ref_fl)['BPM0'].reshape(-1)\n",
    "\n",
    "X1 = []\n",
    "y1 = ground_truth\n",
    "for i in range(0, len(ppg) - window_length + 1, window_shift):\n",
    "\n",
    "    # aggregate accelerometer data into single signal to get the acc window\n",
    "    ppg_window = ppg[i:i+window_length]\n",
    "    acc_window = acc[i:i+window_length]\n",
    "\n",
    "    X1.append(ppg_window)\n",
    "\n",
    "\n",
    "X1 = np.array(X1)\n",
    "print(X1.shape)\n",
    "print(y1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "y = np.array(y)\n",
    "\n",
    "print(data_fls)\n",
    "for i in range(len(data_fls)):\n",
    "\n",
    "    data_fl = data_fls[i]\n",
    "    ref_fl = ref_fls[i]\n",
    "    # load data using LoadTroikaDataFile\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "\n",
    "    # bandpass filter the signals\n",
    "    ppg = bandpass_filter(ppg, fs)\n",
    "    accx = bandpass_filter(accx, fs)\n",
    "    accy = bandpass_filter(accy, fs)\n",
    "    accz = bandpass_filter(accz, fs)\n",
    "\n",
    "    # Consider only magnitude of acceleration\n",
    "    acc = calculate_magnitude(accx, accy, accz)\n",
    "\n",
    "    # Standardization\n",
    "    ppg = (ppg- np.mean(ppg))/np.std(ppg)\n",
    "    acc = (acc- np.mean(acc))/np.std(acc)\n",
    "\n",
    "    # loading the reference file\n",
    "    ground_truth = sp.io.loadmat(ref_fl)['BPM0'].reshape(-1)\n",
    "    y = np.append(y, ground_truth)\n",
    "\n",
    "    for i in range(0, len(ppg) - window_length + 1, window_shift):\n",
    "\n",
    "        # aggregate accelerometer data into single signal to get the acc window\n",
    "        ppg_window = ppg[i:i+window_length]\n",
    "        acc_window = acc[i:i+window_length]\n",
    "\n",
    "        X.append(ppg_window)\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and evaluate\n",
    "seed = 42\n",
    "def split(X, y):\n",
    "    train_size = 0.8\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = split(X, y)\n",
    "\n",
    "# TODO: Check if any of these change the data in any way\n",
    "# Reshaping the array to 3-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 1000, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1000, 1)\n",
    "X = X.reshape(X.shape[0], 1000, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "X = X.astype('float32')\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n",
    "# y = to_categorical(y)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of segments in x_train', x_train.shape[0])\n",
    "print('Number of segments in x_test', x_test.shape[0])\n",
    "\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model (trainX, trainy, testX, testy):\n",
    "trainX, trainy, testX, testy = x_train, y_train, x_test, y_test\n",
    "verbose, epochs, batch_size = 1, 1000, 25 # Batch size used to be 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], 1, 1\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=40, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(filters=32, kernel_size=40, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "oldtime = time.time()\n",
    "model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "# model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "print(\"training time:\")\n",
    "print(time.time()-oldtime)\n",
    "# evaluate model\n",
    "oldtime = time.time()\n",
    "_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\n",
    "predictions = model.predict(testX)\n",
    "testMAE = np.absolute(np.subtract(testy,predictions[:,0])).mean()\n",
    "\n",
    "predictions = model.predict(X)\n",
    "totalMAE = np.absolute(np.subtract(y,predictions[:,0])).mean()\n",
    "\n",
    "print(\"Test, Total MAE:\", str(testMAE), str(totalMAE))\n",
    "print(\"testing time:\")\n",
    "print(time.time()-oldtime)\n",
    "model.summary()\n",
    "\n",
    "# Notes:\n",
    "#   - Flatte, Dense 100, rmsprop Test, Total MAE: 33.09671329490874 33.18895348192425\n",
    "#   - Flatten, Dense 100, adam, Test, Total MAE: 27.908568967205614 28.388384894643007\n",
    "#   - Flatten, Dense 50, then final neuron, Test, Total RMSE: 134.23943523918422 134.3048856681949\n",
    "#   - Just found out I had metrics wrong: Train error came down to 0.735 bpm\n",
    "#   - Test, Total MAE: 2.3047275488171093 1.3646295197764875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)\n",
    "print(predictions[:,0])\n",
    "# print(predictions[:,0]-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network still needs work before I can really scale it to use the entire dataset. Its too noisey even on its own data.\n",
    "fig = plt.figure(figsize=(40, 6))\n",
    "predictions = model.predict(X)\n",
    "# yhat = savgol_filter(predictions[:,0], 71, 3) # window size 51, polynomial order 3\n",
    "totalMAE = np.absolute(np.subtract(y,predictions[:,0])).mean()\n",
    "print(\"Test, Total MAE:\", str(testMAE), str(totalMAE))\n",
    "\n",
    "\n",
    "# aa=pd.DataFrame(predictions)\n",
    "# for i in range(50):\n",
    "#   plt.plot(predictions[:, i])\n",
    "# # plt.plot(y)\n",
    "plt.plot(predictions[:,0])\n",
    "plt.plot(y)\n",
    "# plt.plot(yhat)\n",
    "# X1MAE = np.absolute(np.subtract(y1,predictions)).mean()\n",
    "# plt.title(X1MAE)\n",
    "plt.show\n",
    "\n",
    "\n",
    "a = np.zeros(shape=(9999,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[:, 1].shape)\n",
    "plot_model(model, to_file = proj_dir + 'model_plot.png', show_shapes = True, show_layer_names=True)\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(x_train[0].shape)\n",
    "t = 1200\n",
    "plt.plot(X[t])\n",
    "plt.title(y[t])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
