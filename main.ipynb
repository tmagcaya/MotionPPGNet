{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import time\n",
    "import pandas as pd\n",
    "import math\n",
    "from utils.data_utils import *\n",
    "from utils.viz_utils import *\n",
    "from model.motionPPGNet import motionPPGNet\n",
    "\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import model libs\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Conv1D, Dropout, Flatten, MaxPooling1D, BatchNormalization, LSTM\n",
    "# from keras import optimizers\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "\n",
    "\n",
    "fs = 125\n",
    "minBPM = 40\n",
    "maxBPM = 240\n",
    "window_length = 8 * fs\n",
    "window_shift = 2 * fs  #Overlap = window_length - window_shift\n",
    "\n",
    "# Retrieve dataset files\n",
    "data_dir = \"datasets/troika/training_data\"\n",
    "data_fls, ref_fls = LoadTroikaDataset(data_dir)\n",
    "errs, confs = [], []\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 1000)\n",
      "(148,)\n"
     ]
    }
   ],
   "source": [
    "# Load a single data file and visualize preprocessing\n",
    "data_fl = data_fls[0]\n",
    "ref_fl = ref_fls[0]\n",
    "\n",
    "# load data using LoadTroikaDataFile\n",
    "ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "\n",
    "# bandpass filter the signals\n",
    "ppg = bandpass_filter(ppg, fs)\n",
    "accx = bandpass_filter(accx, fs)\n",
    "accy = bandpass_filter(accy, fs)\n",
    "accz = bandpass_filter(accz, fs)\n",
    "\n",
    "# Consider only magnitude of acceleration\n",
    "acc = calculate_magnitude(accx, accy, accz)\n",
    "\n",
    "# Standardization\n",
    "ppg = (ppg- np.mean(ppg))/np.std(ppg)\n",
    "acc = (acc- np.mean(acc))/np.std(acc)\n",
    "\n",
    "# loading the reference file\n",
    "ground_truth = sp.io.loadmat(ref_fl)['BPM0'].reshape(-1)\n",
    "\n",
    "X1 = []\n",
    "y1 = ground_truth\n",
    "for i in range(0, len(ppg) - window_length + 1, window_shift):\n",
    "\n",
    "    # aggregate accelerometer data into single signal to get the acc window\n",
    "    ppg_window = ppg[i:i+window_length]\n",
    "    acc_window = acc[i:i+window_length]\n",
    "\n",
    "    X1.append(ppg_window)\n",
    "\n",
    "\n",
    "X1 = np.array(X1)\n",
    "print(X1.shape)\n",
    "print(y1.shape)\n",
    "# Visualization Code\n",
    "# single_set_plot(ppg, acc, ground_truth, X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/troika/training_data/DATA_01_TYPE01.mat', 'datasets/troika/training_data/DATA_02_TYPE02.mat', 'datasets/troika/training_data/DATA_03_TYPE02.mat', 'datasets/troika/training_data/DATA_04_TYPE01.mat', 'datasets/troika/training_data/DATA_04_TYPE02.mat', 'datasets/troika/training_data/DATA_05_TYPE02.mat', 'datasets/troika/training_data/DATA_06_TYPE02.mat', 'datasets/troika/training_data/DATA_07_TYPE02.mat', 'datasets/troika/training_data/DATA_08_TYPE02.mat', 'datasets/troika/training_data/DATA_10_TYPE02.mat', 'datasets/troika/training_data/DATA_11_TYPE02.mat', 'datasets/troika/training_data/DATA_12_TYPE02.mat']\n",
      "(1726, 1000)\n",
      "(1726,)\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "X = []\n",
    "y = []\n",
    "y = np.array(y)\n",
    "\n",
    "print(data_fls)\n",
    "for i in range(len(data_fls)):\n",
    "\n",
    "    data_fl = data_fls[i]\n",
    "    ref_fl = ref_fls[i]\n",
    "    # load data using LoadTroikaDataFile\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "\n",
    "    # bandpass filter the signals\n",
    "    ppg = bandpass_filter(ppg, fs)\n",
    "    accx = bandpass_filter(accx, fs)\n",
    "    accy = bandpass_filter(accy, fs)\n",
    "    accz = bandpass_filter(accz, fs)\n",
    "\n",
    "    # Consider only magnitude of acceleration\n",
    "    acc = calculate_magnitude(accx, accy, accz)\n",
    "\n",
    "    # Standardization\n",
    "    ppg = (ppg- np.mean(ppg))/np.std(ppg)\n",
    "    acc = (acc- np.mean(acc))/np.std(acc)\n",
    "\n",
    "    # loading the reference file\n",
    "    ground_truth = sp.io.loadmat(ref_fl)['BPM0'].reshape(-1)\n",
    "    y = np.append(y, ground_truth)\n",
    "\n",
    "    for i in range(0, len(ppg) - window_length + 1, window_shift):\n",
    "\n",
    "        # aggregate accelerometer data into single signal to get the acc window\n",
    "        ppg_window = ppg[i:i+window_length]\n",
    "        acc_window = acc[i:i+window_length]\n",
    "\n",
    "        X.append(ppg_window)\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = TroikaDataset(data_fls, ref_fls, window_length, window_shift, fs)\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_size = 0.8\n",
    "num_train = int(len(dataset) * train_size)\n",
    "num_test = len(dataset) - num_train\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [num_train, num_test])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Visualize a few of the samples\n",
    "# cols, rows = 3, 3\n",
    "# for i in range(1, cols * rows + 1):\n",
    "#     sample_idx = i+(2*(i-1))\n",
    "#     # sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "#     visualize_dataset_sample(dataset, sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose, epochs = 1, 1000\n",
    "n_timesteps, n_features, n_outputs = len(train_dataset[0][0][0]), 1, 1\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "# epochs = 5\n",
    "\n",
    "# Initialize the model\n",
    "model = motionPPGNet(n_timesteps, n_features, n_outputs)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation Loss: 490.29818448153407\n",
      "Epoch 10: Validation Loss: 287.21486871892756\n",
      "Epoch 20: Validation Loss: 204.39492520419034\n",
      "Epoch 30: Validation Loss: 182.79209622469816\n",
      "Epoch 40: Validation Loss: 139.8189267245206\n",
      "Epoch 50: Validation Loss: 120.77021234685725\n",
      "Epoch 60: Validation Loss: 122.7198354547674\n",
      "Epoch 70: Validation Loss: 93.68772472034802\n",
      "Epoch 80: Validation Loss: 104.89927257191052\n",
      "Epoch 90: Validation Loss: 82.73682507601652\n",
      "Epoch 100: Validation Loss: 106.292668429288\n",
      "Epoch 110: Validation Loss: 77.74909695712003\n",
      "Epoch 120: Validation Loss: 195.7204825661399\n",
      "Epoch 130: Validation Loss: 86.32049283114347\n",
      "Epoch 140: Validation Loss: 78.40337163751775\n",
      "Epoch 150: Validation Loss: 78.9636081348766\n",
      "Epoch 160: Validation Loss: 118.3416678688743\n",
      "Epoch 170: Validation Loss: 73.40500068664551\n",
      "Epoch 180: Validation Loss: 95.25776672363281\n",
      "Epoch 190: Validation Loss: 77.29460421475497\n",
      "Epoch 200: Validation Loss: 67.8428525057706\n",
      "Epoch 210: Validation Loss: 65.87679117376155\n",
      "Epoch 220: Validation Loss: 69.15971045060591\n",
      "Epoch 230: Validation Loss: 64.69363802129573\n",
      "Epoch 240: Validation Loss: 64.43311725963245\n",
      "Epoch 250: Validation Loss: 67.75592994689941\n",
      "Epoch 260: Validation Loss: 69.0845999284224\n",
      "Epoch 270: Validation Loss: 60.583561116998844\n",
      "Epoch 280: Validation Loss: 116.08563509854403\n",
      "Epoch 290: Validation Loss: 78.56618361039595\n",
      "Epoch 300: Validation Loss: 70.99723815917969\n",
      "Epoch 310: Validation Loss: 62.79174249822443\n",
      "Epoch 320: Validation Loss: 68.04929455843839\n",
      "Epoch 330: Validation Loss: 63.21150016784668\n",
      "Epoch 340: Validation Loss: 55.415091427889735\n",
      "Epoch 350: Validation Loss: 91.49446938254617\n",
      "Epoch 360: Validation Loss: 87.27712457830256\n",
      "Epoch 370: Validation Loss: 65.24229223077947\n",
      "Epoch 380: Validation Loss: 72.69095108725809\n",
      "Epoch 390: Validation Loss: 67.43375778198242\n",
      "Epoch 400: Validation Loss: 57.884278384121984\n",
      "Epoch 410: Validation Loss: 59.13022752241655\n",
      "Epoch 420: Validation Loss: 68.00396329706365\n",
      "Epoch 430: Validation Loss: 53.091887040571734\n",
      "Epoch 440: Validation Loss: 80.11223012750798\n",
      "Epoch 450: Validation Loss: 76.89355555447665\n",
      "Epoch 460: Validation Loss: 58.40452818437056\n",
      "Epoch 470: Validation Loss: 117.81875991821289\n",
      "Epoch 480: Validation Loss: 67.77728410200639\n",
      "Epoch 490: Validation Loss: 62.717112107710406\n",
      "Epoch 500: Validation Loss: 81.29326248168945\n",
      "Epoch 510: Validation Loss: 59.53038874539462\n",
      "Epoch 520: Validation Loss: 64.5362004366788\n",
      "Epoch 530: Validation Loss: 60.19326400756836\n",
      "Epoch 540: Validation Loss: 76.38845929232511\n",
      "Epoch 550: Validation Loss: 67.84277759898792\n",
      "Epoch 560: Validation Loss: 56.7369797446511\n",
      "Epoch 570: Validation Loss: 95.93015809492631\n",
      "Epoch 580: Validation Loss: 67.3465787714178\n",
      "Epoch 590: Validation Loss: 71.31336004083806\n",
      "Epoch 600: Validation Loss: 99.83912693370472\n",
      "Epoch 610: Validation Loss: 60.38236514004794\n",
      "Epoch 620: Validation Loss: 59.655122930353336\n",
      "Epoch 630: Validation Loss: 60.47017444263805\n",
      "Epoch 640: Validation Loss: 57.74141658436168\n",
      "Epoch 650: Validation Loss: 56.13325517827814\n",
      "Epoch 660: Validation Loss: 51.71613658558238\n",
      "Epoch 670: Validation Loss: 60.11017053777521\n",
      "Epoch 680: Validation Loss: 67.7425032528964\n",
      "Epoch 690: Validation Loss: 97.36439271406694\n",
      "Epoch 700: Validation Loss: 76.342717257413\n",
      "Epoch 710: Validation Loss: 55.18436535921964\n",
      "Epoch 720: Validation Loss: 60.70552565834739\n",
      "Epoch 730: Validation Loss: 81.48135722767223\n",
      "Epoch 740: Validation Loss: 77.51169308749112\n",
      "Epoch 750: Validation Loss: 62.91433767838912\n",
      "Epoch 760: Validation Loss: 76.87190055847168\n",
      "Epoch 770: Validation Loss: 65.06296313892712\n",
      "Epoch 780: Validation Loss: 116.8938120061701\n",
      "Epoch 790: Validation Loss: 65.04061837629838\n",
      "Epoch 800: Validation Loss: 62.99804808876731\n",
      "Epoch 810: Validation Loss: 61.43147104436701\n",
      "Epoch 820: Validation Loss: 70.01789040998979\n",
      "Epoch 830: Validation Loss: 65.78022176569158\n",
      "Epoch 840: Validation Loss: 79.74099349975586\n",
      "Epoch 850: Validation Loss: 58.67686236988414\n",
      "Epoch 860: Validation Loss: 68.33331541581587\n",
      "Epoch 870: Validation Loss: 72.3979818170721\n",
      "Epoch 880: Validation Loss: 75.1635697104714\n",
      "Epoch 890: Validation Loss: 55.13676608692516\n",
      "Epoch 900: Validation Loss: 57.219246257435195\n",
      "Epoch 910: Validation Loss: 83.03709376942028\n",
      "Epoch 920: Validation Loss: 64.25415593927556\n",
      "Epoch 930: Validation Loss: 69.0288155295632\n",
      "Epoch 940: Validation Loss: 68.7518802989613\n",
      "Epoch 950: Validation Loss: 97.9056562943892\n",
      "Epoch 960: Validation Loss: 84.69416063482112\n",
      "Epoch 970: Validation Loss: 75.72893541509455\n",
      "Epoch 980: Validation Loss: 82.51695719632235\n",
      "Epoch 990: Validation Loss: 70.69983551718973\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for ppg, acc, targets in train_loader:\n",
    "        inputs, targets = ppg.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:  # Validation every 10 epochs\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for ppg, acc, targets in test_loader:\n",
    "                inputs, targets = ppg.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "            print(f'Epoch {epoch}: Validation Loss: {total_loss / len(test_loader)}')\n",
    "\n",
    "# In TF Code had about 2 MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and evaluate\n",
    "seed = 42\n",
    "def split(X, y):\n",
    "    train_size = 0.8\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = split(X, y)\n",
    "\n",
    "# TODO: Check if any of these change the data in any way\n",
    "# Reshaping the array to 3-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 1000, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1000, 1)\n",
    "X = X.reshape(X.shape[0], 1000, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "X = X.astype('float32')\n",
    "\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n",
    "# y = to_categorical(y)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of segments in x_train', x_train.shape[0])\n",
    "print('Number of segments in x_test', x_test.shape[0])\n",
    "\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model (trainX, trainy, testX, testy):\n",
    "trainX, trainy, testX, testy = x_train, y_train, x_test, y_test\n",
    "verbose, epochs, batch_size = 1, 1000, 25 # Batch size used to be 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], 1, 1\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=40, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(filters=32, kernel_size=40, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "oldtime = time.time()\n",
    "model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "# model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "print(\"training time:\")\n",
    "print(time.time()-oldtime)\n",
    "# evaluate model\n",
    "oldtime = time.time()\n",
    "_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\n",
    "predictions = model.predict(testX)\n",
    "testMAE = np.absolute(np.subtract(testy,predictions[:,0])).mean()\n",
    "\n",
    "predictions = model.predict(X)\n",
    "totalMAE = np.absolute(np.subtract(y,predictions[:,0])).mean()\n",
    "\n",
    "print(\"Test, Total MAE:\", str(testMAE), str(totalMAE))\n",
    "print(\"testing time:\")\n",
    "print(time.time()-oldtime)\n",
    "model.summary()\n",
    "\n",
    "# Notes:\n",
    "#   - Flatte, Dense 100, rmsprop Test, Total MAE: 33.09671329490874 33.18895348192425\n",
    "#   - Flatten, Dense 100, adam, Test, Total MAE: 27.908568967205614 28.388384894643007\n",
    "#   - Flatten, Dense 50, then final neuron, Test, Total RMSE: 134.23943523918422 134.3048856681949\n",
    "#   - Just found out I had metrics wrong: Train error came down to 0.735 bpm\n",
    "#   - Test, Total MAE: 2.3047275488171093 1.3646295197764875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)\n",
    "print(predictions[:,0])\n",
    "# print(predictions[:,0]-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network still needs work before I can really scale it to use the entire dataset. Its too noisey even on its own data.\n",
    "fig = plt.figure(figsize=(40, 6))\n",
    "predictions = model.predict(X)\n",
    "# yhat = savgol_filter(predictions[:,0], 71, 3) # window size 51, polynomial order 3\n",
    "totalMAE = np.absolute(np.subtract(y,predictions[:,0])).mean()\n",
    "print(\"Test, Total MAE:\", str(testMAE), str(totalMAE))\n",
    "\n",
    "\n",
    "# aa=pd.DataFrame(predictions)\n",
    "# for i in range(50):\n",
    "#   plt.plot(predictions[:, i])\n",
    "# # plt.plot(y)\n",
    "plt.plot(predictions[:,0])\n",
    "plt.plot(y)\n",
    "# plt.plot(yhat)\n",
    "# X1MAE = np.absolute(np.subtract(y1,predictions)).mean()\n",
    "# plt.title(X1MAE)\n",
    "plt.show\n",
    "\n",
    "\n",
    "a = np.zeros(shape=(9999,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[:, 1].shape)\n",
    "plot_model(model, to_file = proj_dir + 'model_plot.png', show_shapes = True, show_layer_names=True)\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(x_train[0].shape)\n",
    "t = 1200\n",
    "plt.plot(X[t])\n",
    "plt.title(y[t])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
